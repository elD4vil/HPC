# Sistema HPC Distribuido con Docker Registry

Este proyecto implementa un sistema de computaci√≥n de alto rendimiento (HPC) distribuido usando **Docker Registry** y **Vagrant**, con una arquitectura maestro-esclavo distribuida en m√∫ltiples m√°quinas virtuales para m√°ximo realismo.

## üèóÔ∏è Arquitectura Distribuida

### Topolog√≠a de Red:
```
Registry Local (192.168.56.1:5000)
       ‚Üì
VM Maestro (192.168.56.10)
‚îú‚îÄ‚îÄ Container: maestro (Puerto 5001)
‚îú‚îÄ‚îÄ Container: memoria1 (Puerto 5003)  
‚îî‚îÄ‚îÄ Container: memoria2 (Puerto 5004)
       ‚Üì (Red privada)
VM Esclavo1 (192.168.56.11)        VM Esclavo2 (192.168.56.12)
‚îî‚îÄ‚îÄ Container: esclavo1 (5002)      ‚îî‚îÄ‚îÄ Container: esclavo2 (5002)
```

### Componentes del Sistema:

#### üß† **Nodo Maestro** (VM: 192.168.56.10)
- **Maestro** (Puerto 5001):
  - API REST para gesti√≥n centralizada de trabajos
  - Cola distribuida de trabajos pendientes
  - **Distribuci√≥n inteligente** basada en m√©tricas en tiempo real
  - Monitoreo continuo de salud de esclavos
  - Agregaci√≥n de resultados del cluster

- **Memoria1 & Memoria2** (Puertos 5003-5004):
  - **Monitoreo distribuido** de recursos del cluster
  - Alertas autom√°ticas por saturaci√≥n de recursos
  - M√©tricas centralizadas de CPU/memoria/disco
  - Reportes peri√≥dicos al maestro

#### üí™ **Nodos Esclavos** (VMs: 192.168.56.11-12)
- **Esclavos** (Puerto 5002 cada uno):
  - API REST para recepci√≥n de trabajos
  - **Ejecuci√≥n paralela** de algoritmos computacionales
  - Auto-registro din√°mico con el maestro
  - Reporte en tiempo real de estado y recursos

#### üê≥ **Docker Registry** (192.168.56.1:5000)
- **Distribuci√≥n centralizada** de im√°genes Docker
- Versionado consistente entre todas las VMs
- Despliegue sincronizado del software HPC

## üîÑ Flujo de Trabajo Distribuido

### 1. **Inicializaci√≥n del Cluster**:
   ```
   Registry ‚Üí Distribuci√≥n de im√°genes ‚Üí VMs
   ‚îú‚îÄ‚îÄ VM Maestro: Inicia servicios centrales
   ‚îú‚îÄ‚îÄ VM Esclavo1: Auto-registro con maestro
   ‚îî‚îÄ‚îÄ VM Esclavo2: Auto-registro con maestro
   ```

### 2. **Distribuci√≥n Inteligente de Trabajos**:
   ```
   Cliente ‚Üí Maestro ‚Üí An√°lisis de carga ‚Üí Selecci√≥n de esclavo √≥ptimo
   ```
   - **M√©tricas consideradas**:
     - Carga de CPU por esclavo
     - Memoria disponible en tiempo real
     - Latencia de red entre nodos
     - Alertas de saturaci√≥n de recursos
   - **Algoritmos de selecci√≥n**:
     - Round-robin con balance de carga
     - Priorizaci√≥n por recursos disponibles

### 3. **Ejecuci√≥n Distribuida**:
   ```
   Maestro ‚Üí Esclavo seleccionado ‚Üí Ejecuci√≥n ‚Üí Resultado ‚Üí Maestro
   ```
   - Ejecuci√≥n en **hilos separados** para paralelismo
   - **Tolerancia a fallos** con re-encolado autom√°tico
   - Timeout configurable por trabajo

### 4. **Monitoreo del Cluster**:
   ```
   Nodos Memoria ‚Üí M√©tricas ‚Üí Maestro ‚Üí Dashboard centralizado
   ```
   - Ping distribuido cada 10 segundos
   - Reportes de memoria cada 30 segundos
   - **Alertas autom√°ticas** por saturaci√≥n

## üåê APIs del Sistema Distribuido

### üß† Maestro Centralizado (http://192.168.56.10:5001)

#### **Gesti√≥n de Trabajos**:
- `POST /agregar_trabajo` - Agregar trabajo individual al cluster
- `POST /cargar_trabajos` - Carga masiva desde archivo
- `POST /limpiar_cola` - Limpieza de cola distribuida
- `GET /estado` - **Estado completo del cluster**

#### **Gesti√≥n de Cluster**:
- `GET /esclavos` - Lista de nodos esclavos activos
- `GET /memoria` - **M√©tricas agregadas del cluster**
- `POST /registrar_esclavo` - Auto-registro de nuevos nodos
- `POST /resultado` - Recepci√≥n de resultados distribuidos

### üí™ Esclavos Distribuidos (192.168.56.11:5002, 192.168.56.12:5002)

- `GET /ping` - **M√©tricas de nodo** (CPU, memoria, carga)
- `POST /ejecutar` - Ejecuci√≥n de trabajo asignado
- `GET /estado` - Estado detallado del nodo esclavo

### üìä Nodos de Memoria (192.168.56.10:5003-5004)

- `GET /metricas` - **M√©tricas en tiempo real**
- `GET /historial` - Historial de rendimiento
- `GET /alertas` - **Alertas activas del cluster**

## üöÄ Despliegue del Sistema Distribuido

### **Opci√≥n 1: Despliegue Automatizado Completo**

```bash
# 1. Construir y publicar im√°genes en registry
./build-all-images.sh

# 2. Crear cluster completo (4 VMs + Registry)
./full-deploy.sh

# 3. Monitorear despliegue
vagrant status
```

### **Opci√≥n 2: Despliegue Manual Paso a Paso**

```bash
# 1. Crear Docker Registry local
docker run -d -p 5000:5000 --name registry registry:2

# 2. Construir y publicar im√°genes
docker build -t localhost:5000/hpc-maestro:latest ./maestro
docker build -t localhost:5000/hpc-memoria:latest ./memoria
docker build -t localhost:5000/hpc-esclavo:latest ./esclavo

docker push localhost:5000/hpc-maestro:latest
docker push localhost:5000/hpc-memoria:latest
docker push localhost:5000/hpc-esclavo:latest

# 3. Crear cluster de VMs
vagrant up

# 4. Desplegar servicios distribuidos
./deploy-distributed.sh
```

### **Opci√≥n 3: Despliegue con PowerShell (Windows)**

```powershell
# Despliegue completo desde PowerShell
.\deploy-distributed.ps1
```

## üìã Gesti√≥n de Trabajos Distribuidos

### **Comandos del Cliente HPC**:

```bash
# Estado completo del cluster distribuido
python cliente_hpc.py --comando estado

# Carga masiva de trabajos al cluster  
python cliente_hpc.py --comando cargar_trabajos --archivo trabajos.txt

# Trabajo individual con distribuci√≥n autom√°tica
python cliente_hpc.py --comando trabajo --algoritmo juliaCython --width 2000 --height 1500 --iteraciones 500

# Monitoreo continuo del cluster
python cliente_hpc.py --comando monitorear --intervalo 10

# Demostraci√≥n del sistema distribuido
python demo_trabajos.py
```

### **Formato de Archivo de Trabajos** (`maestro/trabajos.txt`):

```
# Trabajos de alta prioridad
Secuencial,3000,2400,1000,1
Numpy,3200,2500,1200,2
juliaCython,3500,2800,1500,1

# Trabajos de estr√©s para cluster
Secuencial,4000,3000,2000,3
Numpy,4500,3500,2500,2
juliaCython,5000,4000,3000,1
```

## üîß Caracter√≠sticas Avanzadas del Sistema Distribuido

### **Distribuci√≥n Inteligente**:
- **Balanceador de carga** basado en m√©tricas reales
- **Selecci√≥n din√°mica** del nodo √≥ptimo
- **Prevenci√≥n de saturaci√≥n** mediante alertas

### **Alta Disponibilidad**:
- **Tolerancia a fallos** de nodos individuales
- **Re-encolado autom√°tico** de trabajos fallidos
- **Recuperaci√≥n autom√°tica** de nodos

### **Escalabilidad Horizontal**:
- **Adici√≥n din√°mica** de nuevos nodos esclavos
- **Auto-descubrimiento** de servicios
- **Particionamiento autom√°tico** de carga

### **Monitoreo Distribuido**:
- **M√©tricas agregadas** de todo el cluster
- **Dashboards centralizados** de rendimiento
- **Alertas proactivas** por degradaci√≥n

## üåê Arquitectura de Red

### **Configuraci√≥n de Red Privada**:
```
192.168.56.0/24 (Red privada Vagrant)
‚îú‚îÄ‚îÄ 192.168.56.1:5000   ‚Üí Docker Registry
‚îú‚îÄ‚îÄ 192.168.56.10:5001  ‚Üí VM Maestro (+ memoria1,2)
‚îú‚îÄ‚îÄ 192.168.56.11:5002  ‚Üí VM Esclavo1
‚îî‚îÄ‚îÄ 192.168.56.12:5002  ‚Üí VM Esclavo2
```

### **Acceso desde Host**:
- **Maestro**: http://192.168.56.10:5001
- **Registry**: http://localhost:5000
- **SSH a nodos**: `vagrant ssh maestro|esclavo1|esclavo2`

## üìä Monitoreo y M√©tricas

### **Comandos de Monitoreo**:

```bash
# Ver estado de todo el cluster
vagrant status

# Logs del maestro en tiempo real
vagrant ssh maestro -c "docker-compose logs -f maestro"

# M√©tricas de un esclavo espec√≠fico
vagrant ssh esclavo1 -c "curl http://localhost:5002/ping"

# Conectividad entre nodos
vagrant ssh esclavo1 -c "ping -c 3 192.168.56.10"

# Trabajos completados
vagrant ssh maestro -c "cat /vagrant/maestro/resultados.txt | tail -10"
```

### **M√©tricas del Sistema**:
- **CPU**: Utilizaci√≥n por nodo y agregada
- **Memoria**: Uso actual y disponible
- **Red**: Latencia entre nodos
- **Trabajos**: Cola, completados, fallidos
- **Alertas**: Saturaci√≥n, timeouts, errores

## üõ†Ô∏è Gesti√≥n del Cluster

### **Comandos de Gesti√≥n**:

```bash
# Reiniciar cluster completo
vagrant reload

# Reiniciar nodo espec√≠fico
vagrant reload esclavo1

# Actualizar im√°genes en cluster
docker push localhost:5000/hpc-maestro:latest
vagrant ssh maestro -c "docker-compose pull && docker-compose up -d"

# Escalar cluster (agregar esclavo)
# Modificar Vagrantfile y ejecutar:
vagrant up esclavo3
```

### **Troubleshooting Distribuido**:

```bash
# Verificar registry
curl http://localhost:5000/v2/_catalog

# Verificar conectividad cluster
vagrant ssh maestro -c "ping -c 2 192.168.56.11"
vagrant ssh maestro -c "ping -c 2 192.168.56.12"

# Ver logs de despliegue
vagrant ssh maestro -c "docker-compose logs"

# Reiniciar servicios espec√≠ficos
vagrant ssh maestro -c "docker-compose restart maestro"
```

## üìà Optimizaciones de Rendimiento

### **Configuraci√≥n de VMs**:
- **Maestro**: 2 CPU, 2GB RAM (servicios centrales)
- **Esclavos**: 1 CPU, 1GB RAM (ejecuci√≥n optimizada)

### **Optimizaciones de Red**:
- Red privada dedicada para comunicaci√≥n cluster
- Registry local para distribuci√≥n r√°pida de im√°genes

### **Algoritmos Optimizados**:
- **Secuencial**: Implementaci√≥n b√°sica Python
- **Numpy**: Optimizaci√≥n vectorizada
- **juliaCython**: Compilaci√≥n JIT para m√°ximo rendimiento

## üéØ Casos de Uso

### **Desarrollo y Testing**:
- Simulaci√≥n de entornos de producci√≥n distribuidos
- Testing de tolerancia a fallos
- Validaci√≥n de algoritmos paralelos

### **Educaci√≥n**:
- Demostraci√≥n de conceptos HPC
- Pr√°ctica con sistemas distribuidos
- Monitoreo de recursos en tiempo real

### **Prototipado**:
- Validaci√≥n de arquitecturas distribuidas
- Benchmarking de algoritmos
- Pruebas de escalabilidad

---

**Sistema HPC Distribuido - Computaci√≥n de Alto Rendimiento con Docker Registry y